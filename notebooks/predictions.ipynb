{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Predictive models\n",
    "\n",
    "In this notebook, we build the prediction dataframes for transfer success probability using SBB Historical data. We then use the models to make a transfer success prediction function and test it.\n",
    "\n",
    "The sbb is loaded,  filtered, and addapted to remove missing data, trips outside of the considered schedule, and stops ouside of zurich areas.\n",
    "\n",
    "The goal is to extract delays average and standard deviation for a specific trip, station and time. \n",
    "\n",
    "Table of contents of this notebook:\n",
    "\n",
    "1. [Helper functions](#1.-Helper-functions)\n",
    "\n",
    "Define useful library and functions to be used\n",
    "\n",
    "2. [Data preprocessing](#2.-Data-preprocessing)\n",
    "\n",
    "The sbb is loaded,  filtered, and addapted to remove missing data, trips outside of the considered schedule, and stops ouside of zurich areas.\n",
    "\n",
    "3. [Build prediction dataframe](#3.-Build-prediction-dataframe)\n",
    "\n",
    "Extract delays average and standard deviation for different sets of parameters (options: StationId, TripId, DayPeriod). \n",
    "\n",
    "4. [Transfer probability function](#4.-Transfer-probability-function)\n",
    "\n",
    "Create a function that, given transfer infomation such as arrival and departure details, compute a probability that the transfer succeeds\n",
    "\n",
    "5. [Validation](#5.-Validation)\n",
    "\n",
    "Test the function for a few values to see it it work properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.app.name': '272648_final'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>9244</td><td>application_1589299642358_3813</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3813/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3813_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9250</td><td>application_1589299642358_3819</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3819/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3819_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9286</td><td>application_1589299642358_3857</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3857/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3857_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9292</td><td>application_1589299642358_3866</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3866/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3866_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9294</td><td>application_1589299642358_3868</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3868/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3868_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9295</td><td>application_1589299642358_3869</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3869/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3869_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9300</td><td>application_1589299642358_3877</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3877/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3877_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9302</td><td>application_1589299642358_3879</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3879/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3879_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9303</td><td>application_1589299642358_3880</td><td>pyspark</td><td>dead</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/cluster/app/application_1589299642358_3880\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster054.iccluster.epfl.ch:8188/applicationhistory/logs/iccluster070.iccluster.epfl.ch:45454/container_e06_1589299642358_3880_01_000001/container_e06_1589299642358_3880_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9308</td><td>application_1589299642358_3885</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3885/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3885_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9311</td><td>application_1589299642358_3888</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3888/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3888_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9312</td><td>application_1589299642358_3889</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3889/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3889_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9313</td><td>application_1589299642358_3890</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3890/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3890_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9314</td><td>application_1589299642358_3891</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3891/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3891_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9315</td><td>application_1589299642358_3892</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3892/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3892_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9317</td><td>application_1589299642358_3894</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3894/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3894_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9318</td><td>application_1589299642358_3895</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3895/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3895_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9319</td><td>application_1589299642358_3896</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3896/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3896_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9320</td><td>application_1589299642358_3897</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3897/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3897_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9321</td><td>application_1589299642358_3898</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3898/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3898_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9322</td><td>application_1589299642358_3899</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3899/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3899_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9323</td><td>application_1589299642358_3900</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3900/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3900_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9324</td><td>application_1589299642358_3901</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3901/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3901_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9325</td><td>application_1589299642358_3902</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3902/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3902_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9326</td><td>application_1589299642358_3903</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3903/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3903_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9327</td><td>application_1589299642358_3904</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3904/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3904_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9328</td><td>application_1589299642358_3905</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3905/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3905_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9329</td><td>application_1589299642358_3906</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3906/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3906_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9330</td><td>application_1589299642358_3907</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3907/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3907_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9331</td><td>application_1589299642358_3909</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3909/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3909_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9332</td><td>application_1589299642358_3910</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3910/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3910_01_000001/ebouille\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{\"conf\": {\n",
    "    \"spark.app.name\": \"272648_final\"\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>9159</td><td>application_1589299642358_3724</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3724/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3724_01_000001/ebouille\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fa05d508450>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "import pandas as pd\n",
    "from geopy import distance as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import subprocess, pickle\n",
    "\n",
    "# Hdfs helper methods\n",
    "def run_cmd(args_list):\n",
    "    \"\"\"Run linux commands.\"\"\"\n",
    "    print('Running system command: {0}'.format(' '.join(args_list)))    \n",
    "    proc = subprocess.Popen(args_list,                            \n",
    "                            stdout=subprocess.PIPE,                            \n",
    "                            stderr=subprocess.PIPE)    \n",
    "    s_output, s_err = proc.communicate()    \n",
    "    s_return =  proc.returncode\n",
    "    return s_return, s_output, s_err\n",
    "\n",
    "def save_hdfs(localPath, hdfsPath):\n",
    "    \n",
    "    (ret, out, err)= run_cmd(['hdfs','dfs','-put','-f', localPath, hdfsPath])\n",
    "    if err:\n",
    "        print(err)\n",
    "    else:\n",
    "        print('Success')\n",
    "        \n",
    "def read_hdfs(hdfsPath):\n",
    "    \n",
    "    (ret, out, err)= run_cmd(['hdfs','dfs','-cat', hdfsPath])\n",
    "    if err:\n",
    "        print(err)\n",
    "    else:\n",
    "        print('Success')\n",
    "    return pickle.loads(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hdfs dfs -cat /user/lortkipa/filtered_stops_Premoved.pkl\n",
      "Success"
     ]
    }
   ],
   "source": [
    "# Load the zurich area stops\n",
    "stops = read_hdfs('/user/lortkipa/filtered_stops_Premoved.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- betriebstag: string (nullable = true)\n",
      " |-- fahrt_bezeichner: string (nullable = true)\n",
      " |-- betreiber_id: string (nullable = true)\n",
      " |-- betreiber_abk: string (nullable = true)\n",
      " |-- betreiber_name: string (nullable = true)\n",
      " |-- produkt_id: string (nullable = true)\n",
      " |-- linien_id: string (nullable = true)\n",
      " |-- linien_text: string (nullable = true)\n",
      " |-- umlauf_id: string (nullable = true)\n",
      " |-- verkehrsmittel_text: string (nullable = true)\n",
      " |-- zusatzfahrt_tf: string (nullable = true)\n",
      " |-- faellt_aus_tf: string (nullable = true)\n",
      " |-- bpuic: string (nullable = true)\n",
      " |-- haltestellen_name: string (nullable = true)\n",
      " |-- ankunftszeit: string (nullable = true)\n",
      " |-- an_prognose: string (nullable = true)\n",
      " |-- an_prognose_status: string (nullable = true)\n",
      " |-- abfahrtszeit: string (nullable = true)\n",
      " |-- ab_prognose: string (nullable = true)\n",
      " |-- ab_prognose_status: string (nullable = true)\n",
      " |-- durchfahrt_tf: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "# Load sbb dataset\n",
    "sbb = spark.read.orc('/data/sbb/orc/istdaten')\n",
    "sbb.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter useless columns\n",
    "df = sbb.select(\n",
    "    F.to_timestamp(sbb['betriebstag'], \"dd.MM.yyyy\").alias('trip_date'),\n",
    "    sbb['fahrt_bezeichner'].alias('trip_id'),\n",
    "    sbb['produkt_id'].alias('transport_type'),\n",
    "   # sbb['linien_id'].alias('train_number'),\n",
    "    sbb['haltestellen_name'].alias('stop_name'),\n",
    "    F.to_timestamp(sbb['ankunftszeit'], \"dd.MM.yyyy HH:mm\").alias('schedule_arrival_time'),\n",
    "    F.to_timestamp(sbb['an_prognose'], \"dd.MM.yyyy HH:mm:ss\").alias('true_arrival_time'),\n",
    "    sbb['an_prognose_status'].alias('status_arrival_time'),\n",
    "    F.to_timestamp(sbb['abfahrtszeit'], \"dd.MM.yyyy HH:mm\").alias('schedule_departure_time'),\n",
    "    F.to_timestamp(sbb['ab_prognose'], \"dd.MM.yyyy HH:mm:ss\").alias('true_departure_time'),\n",
    "    sbb['ab_prognose_status'].alias('status_departure_time'),\n",
    "    sbb['durchfahrt_tf'].alias('is_not_stopping')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to devide days in 4 periods using our analysis in homework1: \n",
    "1. Morning rush hour: 7h -> 10h\n",
    "2. Mid day period: 10h -> 16h\n",
    "3. Evening rush hour: 16h -> 19h\n",
    "4. Late day: 19h -> 23h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Return a day category using trip hour\n",
    "def get_day_period(hour):\n",
    "    if (hour < 10):\n",
    "        return 1\n",
    "    elif (hour < 16):\n",
    "        return 2\n",
    "    elif (hour < 19):\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "    \n",
    "from pyspark.sql.types import IntegerType\n",
    "get_day_period_udf = F.udf(lambda h: get_day_period(h), IntegerType())\n",
    "    \n",
    "    \n",
    "# Return stop id given stop name\n",
    "@udf('string')\n",
    "def get_stop_id(stop_name):\n",
    "    return stops_name_to_id[stop_name]['stop_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter the data with data with GESCHAETZ\n",
    "df = df.filter((df['status_arrival_time'] == 'GESCHAETZT') & (df['status_departure_time'] == 'GESCHAETZT'))\n",
    "\n",
    "# Consider only data with reasonable hours of the day: 7h -> 23h\n",
    "df = df.filter((F.hour(df['schedule_arrival_time']) < 23) & (F.hour(df['schedule_departure_time']) > 6))\n",
    "\n",
    "# Remove all data from trips happening in Saturday and Sunday (we will only consider May 13-May 17 2019 schedule so do not consider the week end).\n",
    "df = df.withColumn('weekday', F.date_format(df['schedule_arrival_time'], 'u'))\n",
    "df = df.filter(df['weekday'] < 6) # Keep only monday to friday\n",
    "\n",
    "# Remove the trip is the train is not stopping\n",
    "df = df.filter(df['is_not_stopping'] == 'false')\n",
    "\n",
    "from pyspark.sql.types import LongType\n",
    "# Add the delay columns\n",
    "df = df.withColumn('arrival_delay', df['true_arrival_time'].cast(LongType()) - df['schedule_arrival_time'].cast(LongType()))\n",
    "df = df.withColumn('departure_delay', df['true_departure_time'].cast(LongType()) - df['schedule_departure_time'].cast(LongType()))\n",
    "\n",
    "# Remove outliers for delays (remove all delay greater than 1h)\n",
    "df = df.filter((df['arrival_delay'] < 3600) & (df['arrival_delay'] > -3600) & (df['departure_delay'] < 3600) & (df['departure_delay'] > -3600))\n",
    "\n",
    "# Remove data about useless stations\n",
    "df = df.filter(df['stop_name'].isin(stops['stop_name'].values.tolist()))\n",
    "\n",
    "# Add the day period column to the data\n",
    "df = df.withColumn('day_period', get_day_period_udf(F.hour(df['schedule_departure_time'])))\n",
    "\n",
    "# Add the stopId corresponding to the stop name.\n",
    "stops_name_to_id = stops.reset_index().set_index('stop_name')[['stop_id']].T.to_dict()\n",
    "df = df.withColumn('stop_id', get_stop_id(df['stop_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove now useless columns\n",
    "df_cleaned = df.select('trip_date', 'weekday', 'trip_id', 'transport_type', 'stop_id', 'day_period', 'schedule_arrival_time', 'arrival_delay', 'schedule_departure_time', 'departure_delay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have cleaned the historical data to keep only what we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------------+--------------+-------+----------+---------------------+-------------+-----------------------+---------------+\n",
      "|          trip_date|weekday|       trip_id|transport_type|stop_id|day_period|schedule_arrival_time|arrival_delay|schedule_departure_time|departure_delay|\n",
      "+-------------------+-------+--------------+--------------+-------+----------+---------------------+-------------+-----------------------+---------------+\n",
      "|2018-04-16 00:00:00|      1|85:11:1252:001|           Zug|8503000|         4|  2018-04-16 21:23:00|          -75|    2018-04-16 21:36:00|             67|\n",
      "|2018-04-16 00:00:00|      1|85:11:1255:001|           Zug|8503000|         1|  2018-04-16 08:26:00|          132|    2018-04-16 08:37:00|            118|\n",
      "|2018-04-16 00:00:00|      1|85:11:1509:003|           Zug|8503000|         1|  2018-04-16 07:30:00|           45|    2018-04-16 07:39:00|             29|\n",
      "|2018-04-16 00:00:00|      1|85:11:1509:003|           Zug|8503016|         1|  2018-04-16 07:49:00|          170|    2018-04-16 07:51:00|            190|\n",
      "|2018-04-16 00:00:00|      1|85:11:1510:003|           Zug|8503016|         1|  2018-04-16 07:11:00|          125|    2018-04-16 07:13:00|            178|\n",
      "+-------------------+-------+--------------+--------------+-------+----------+---------------------+-------------+-----------------------+---------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build prediction dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build day period prediction dataframe (in case full / stop predictions are missing)\n",
    "period_prediction_delays = df_cleaned.groupby('day_period').agg(F.mean('arrival_delay').alias('mean_arrival_delay'), F.stddev('arrival_delay').alias('std_arrival_delay'),\n",
    "                                                              F.mean('departure_delay').alias('mean_departure_delay'), F.stddev('departure_delay').alias('std_departure_delay'))\n",
    "period_prediction_delays = period_prediction_delays.collect()\n",
    "period_prediction_df = pd.DataFrame(period_prediction_delays, columns=['day_period', 'mean_arrival_delay', 'std_arrival_delay', 'mean_departure_delay', 'std_departure_delay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250e5ea1cb364b18abc3f664d56eacee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a stop prediction (in case full prediction missing)\n",
    "stop_prediction_delays = df_cleaned.groupby('day_period', 'stop_id').agg(F.mean('arrival_delay').alias('mean_arrival_delay'), F.stddev('arrival_delay').alias('std_arrival_delay'),\n",
    "                                                              F.mean('departure_delay').alias('mean_departure_delay'), F.stddev('departure_delay').alias('std_departure_delay'))\n",
    "stop_prediction_delays = stop_prediction_delays.collect()\n",
    "stop_prediction_df = pd.DataFrame(stop_prediction_delays, columns=['day_period', 'stop_id', 'mean_arrival_delay', 'std_arrival_delay', 'mean_departure_delay', 'std_departure_delay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a full prediction (using trip_id, stop_id, and day_period)\n",
    "full_prediction_delays = df_cleaned.groupby('trip_id' ,'stop_id', 'day_period').agg(F.mean('arrival_delay').alias('mean_arrival_delay'), F.stddev('arrival_delay').alias('std_arrival_delay'),\n",
    "                                                              F.mean('departure_delay').alias('mean_departure_delay'), F.stddev('departure_delay').alias('std_departure_delay'))\n",
    "full_prediction_delays = full_prediction_delays.collect()\n",
    "full_prediction_df = pd.DataFrame(full_prediction_delays, columns = ['trip_id', 'stop_id','day_period', 'mean_arrival_delay', 'std_arrival_delay', 'mean_departure_delay', 'std_departure_delay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to pickle file\n",
    "period_prediction_df.to_pickle('period_prediction_df.pkl')\n",
    "stop_prediction_df.to_pickle('stop_prediction_df.pkl')\n",
    "full_prediction_df.to_pickle('full_prediction_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to hdfs\n",
    "save_hdfs('period_prediction_df.pkl','/user/lortkipa/')\n",
    "save_hdfs('stop_prediction_df.pkl','/user/lortkipa/')\n",
    "save_hdfs('full_prediction_df.pkl','/user/lortkipa/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer probability function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from geopy import distance as dist\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import math\n",
    "\n",
    "# Compute the transfer success probability given transfer parameters\n",
    "def transfer_success_probability(stop1_id, trip1_id, trip1_arrival_time, stop2_id, trip2_id, trip2_departure_time,  verbose=False, norm=True):\n",
    "    # Extract time in seconds and day period from time string\n",
    "    trip1_arr_seconds, trip1_day_period = get_time_info(trip1_arrival_time)\n",
    "    trip2_dep_seconds, trip2_day_period = get_time_info(trip2_departure_time)\n",
    "\n",
    "    schedule_difference = trip2_dep_seconds - trip1_arr_seconds\n",
    "    \n",
    "    # If the tansfer is more than an hour, return 1 (optimization)\n",
    "    if schedule_difference > 3600: return 1\n",
    "    \n",
    "    minimum_transfer_time = 120\n",
    "    walking_time = compute_walk_time(stop1_id, stop2_id)\n",
    "\n",
    "    # Compute the transfer time surplus\n",
    "    transfer_time_surplus = schedule_difference - walking_time - minimum_transfer_time\n",
    "\n",
    "    # Exctract prediction parameters\n",
    "    avg_arrival_delay, std_arrival_delay = get_prediction_data(stop1_id, trip1_id, trip1_day_period, is_arrival=True)\n",
    "    \n",
    "    if trip2_id == 'Terminus':\n",
    "        avg_departure_delay = 0\n",
    "        std_departure_delay = 0\n",
    "        transfer_time_surplus += minimum_transfer_time # Do not require the mininum two minutes for terminus transfer\n",
    "    else:\n",
    "        avg_departure_delay, std_departure_delay = get_prediction_data(stop2_id, trip2_id, trip2_day_period, is_arrival=False)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Schedule difference: {}\".format(schedule_difference))\n",
    "        print(\"Walking time: {}\".format(walking_time))\n",
    "        print(\"Transfer_time_surplus: {}\".format(transfer_time_surplus))\n",
    "        print(\"Mean arrival delay: {}\".format(avg_arrival_delay))\n",
    "        print(\"Mean departure delay: {}\".format(avg_departure_delay))\n",
    "       \n",
    "    # Compute the success probability\n",
    "    if norm:\n",
    "        return compute_uncertainty_norm(avg_arrival_delay, std_arrival_delay, avg_departure_delay, std_departure_delay, transfer_time_surplus)\n",
    "    else:\n",
    "        return compute_uncertainty(avg_arrival_delay, avg_departure_delay, transfer_time_surplus)\n",
    "\n",
    "# Compute transfer probability for terminus arrival\n",
    "def terminus_success_probability(last_stop_id,\n",
    "                                 last_trip_id,\n",
    "                                 last_trip_arrival_time,\n",
    "                                 terminus_stop_id,\n",
    "                                 terminus_time,\n",
    "                                 verbose=False):\n",
    "    return transfer_success_probability(last_stop_id,\n",
    "                                        last_trip_id,\n",
    "                                        last_trip_arrival_time,\n",
    "                                        terminus_stop_id,\n",
    "                                        'Terminus',\n",
    "                                        terminus_time,\n",
    "                                        verbose)\n",
    "\n",
    "# Get period of day given trip hour\n",
    "def get_day_period(hour):\n",
    "    if hour < 10:\n",
    "        return 1\n",
    "    elif hour < 16:\n",
    "        return 2\n",
    "    elif hour < 19:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "\n",
    "# Compute the duration to walk between two stops\n",
    "def compute_walk_time(stop_id_1, stop_id_2):\n",
    "    \n",
    "    stop1 = stops[stops['stop_id'] == stop_id_1]\n",
    "    \n",
    "    if len(stop1) == 0:\n",
    "        print('Stop1 is missing, can\\'t compute walk time for {}'.format(stop_id_1))\n",
    "        return 0\n",
    "    \n",
    "    lat1 = float(stop1['stop_lat'].values[0])\n",
    "    lon1 = float(stop1['stop_lon'].values[0])\n",
    "    \n",
    "\n",
    "    stop2 = stops[stops['stop_id'] == stop_id_2]\n",
    "    \n",
    "    if len(stop2) == 0:\n",
    "        print('Stop2 is missing, can\\'t compute walk time for {}'.format(stop_id_2))\n",
    "        return 0\n",
    "    \n",
    "    lat2 = float(stop2['stop_lat'].values[0])\n",
    "    lon2 = float(stop2['stop_lon'].values[0])\n",
    "\n",
    "    return dist.distance((lat1, lon1), (lat2, lon2)).km * 1200  # 1200 secondes to make 1 km (corresponds to 50m/min)\n",
    "\n",
    "# Return day period and time in seconds from string time\n",
    "def get_time_info(time_string):\n",
    "    \"\"\"\n",
    "    Given time_string in the form of hh:mm:ss, computes total number of seconds\n",
    "    \"\"\"\n",
    "    split = time_string.split(':')\n",
    "    seconds = int(split[0]) * 3600 + int(split[1]) * 60 + int(split[2])\n",
    "    day_period = get_day_period(int(split[0]))\n",
    "\n",
    "    return seconds, day_period\n",
    "\n",
    "# Load file load pickle / hdfs\n",
    "def get_pred_file(hdfs_path):\n",
    "    file_name = hdfs_path.split(\"/\")[-1]\n",
    "    if path.exists(file_name):\n",
    "        return pd.read_pickle(file_name)\n",
    "    else:\n",
    "        loaded_df = pd.DataFrame(read_hdfs(hdfs_path))\n",
    "        loaded_df.to_pickle(file_name)\n",
    "        return pd.DataFrame(loaded_df)\n",
    "\n",
    "# Extract prediction parameters from adapted prediction dataset\n",
    "def get_prediction_data(stop_id, trip_id, day_period, is_arrival):\n",
    "\n",
    "    delay_column = ['mean_arrival_delay', 'std_arrival_delay'] if is_arrival else ['mean_departure_delay','std_departure_delay']\n",
    "\n",
    "    prediction_param = full_pred[(full_pred['stop_id'] == stop_id)\n",
    "                                 & (full_pred['trip_id'] == trip_id)\n",
    "                                 & (full_pred['day_period'] == day_period)][delay_column]\n",
    "\n",
    "    if len(prediction_param) == 0:\n",
    "        prediction_param = stop_pred[(stop_pred['stop_id'] == stop_id)\n",
    "                                     & (stop_pred['day_period'] == day_period)][delay_column]\n",
    "\n",
    "    if len(prediction_param) == 0:\n",
    "        prediction_param = period_pred[(period_pred['day_period'] == day_period)][delay_column]\n",
    "\n",
    "    return prediction_param.values[0]\n",
    "\n",
    "\n",
    "# Compute uncertainty for exponential random variable approximation\n",
    "def compute_uncertainty(avg_arr_delay, avg_dep_delay, transfer_time_surplus):\n",
    "        \n",
    "    if transfer_time_surplus <= 0:\n",
    "        if avg_dep_delay == 0:\n",
    "            return 0\n",
    "        return avg_dep_delay / (avg_arr_delay + avg_dep_delay) * math.exp(transfer_time_surplus / avg_dep_delay)\n",
    "    else:\n",
    "        if avg_arr_delay == 0:\n",
    "            return 1\n",
    "        return 1 - avg_arr_delay / (avg_arr_delay + avg_dep_delay) * math.exp(-transfer_time_surplus / avg_arr_delay)\n",
    "\n",
    "from scipy.stats import norm\n",
    "# Compute uncertainty for normal random variable approximation\n",
    "def compute_uncertainty_norm(avg_arr_delay, std_arr_delay, avg_dep_delay, std_dep_delay, transfer_time_surplus):\n",
    "    \n",
    "    avg_time_loss = avg_arr_delay - avg_dep_delay\n",
    "    std_time_loss = math.sqrt(math.pow(avg_arr_delay,2)  + math.pow(avg_dep_delay,2))\n",
    "    \n",
    "    return norm.cdf(transfer_time_surplus, avg_time_loss, std_time_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters:\n",
    "\n",
    "##### IN GENERAL:\n",
    "\n",
    "- stop1_id: String - Id of the last arrival stop of current trip\n",
    "- trip1_id: String - Id of the current trip\n",
    "- trip1_arrival_time: String - Time of arrival at stop1 of trip1 (format: 'hh:mm:ss')\n",
    "\n",
    "- stop2_id: String - Id of the first departure stop of the next trip\n",
    "- trip2_id: String - Id of the next trip\n",
    "- trip2_departure_time: String - Time of departure of trip2 at stop2 (format: 'hh:mm:ss').\n",
    "\n",
    "##### FOR TERMINUS:\n",
    "\n",
    "- last_stop_id: String - Id of the last arrival stop of current trip\n",
    "- last_trip_id: String - Id of the current trip\n",
    "- last_trip_arrival_time: String - Time of arrival at stop1 of trip1 (format: 'hh:mm:ss')\n",
    "\n",
    "- terminus_stop_id: String - Id defined by the user as journey end station\n",
    "- terminus_time: Time defined by the user as journey arrival time (format: 'hh:mm:ss')\n",
    "\n",
    "#### Examples:\n",
    "\n",
    "__Case A: Normal transfer__\n",
    "\n",
    "__A.1. Normal transfer without walk__\n",
    "\n",
    "[(u'8503006', '08:18:00', u'229.TA.26-14-j19-1.43.H', '00:03:00'), <br> \n",
    "(u'8503129', '08:29:00', u'51.TA.26-19-j19-1.16.H', '00:02:00'),]\n",
    "\n",
    "*transfer_success_probability(<br> \n",
    "stop1_id = u'8503129',<br> \n",
    "trip1_id = u'229.TA.26-14-j19-1.43.H',<br> \n",
    "trip1_arrival_time = '08:21:00',<br> \n",
    "stop2_id = u'8503129',<br> \n",
    "trip2_id = u'51.TA.26-19-j19-1.16.H',<br> \n",
    "trip2_departure_time = '08:29:00')*\n",
    "\n",
    "u'8503006' station is not the arrival station of trip1, it is it's departure. In this case, \n",
    "the arrival station of trip1 is the same as the departure station of trip2\n",
    "\n",
    "__A.2. Normal transfer with walk__\n",
    "\n",
    "[(u'8503305', '08:47:00', u'39.TA.26-7-A-j19-1.12.H', '00:03:00'), <br>\n",
    "(u'8503304', '09:11:42', 'walk', '00:00:17'), <br>\n",
    "(u'8575944', '09:14:00', u'156.TA.26-650-j19-1.7.H', '00:02:00')]\n",
    "\n",
    "__Case B: Arrival at terminus__\n",
    "\n",
    "\n",
    "__B.1. Terminus transfer without walk__\n",
    "\n",
    "[(u'8503381', '09:34:00', u'162.TA.26-655-j19-1.12.R', '00:03:00'), <br>\n",
    "(u'8503376', '10:00:00', 'Terminus', '00:00:00')]\n",
    "\n",
    "\n",
    "terminus_success_probability(<br> \n",
    "last_stop_id = u'8503376',<br> \n",
    "last_trip_id = u'39.TA.26-7-A-j19-1.12.H',<br> \n",
    "last_trip_arrival_time = '09:37:00',<br> \n",
    "terminus_stop_id = u'8503376',<br> \n",
    "terminus_time = '10:00:00')\n",
    "\n",
    "__B.1. Terminus transfer without walk__\n",
    "\n",
    "[(u'8503381', '09:34:00', u'162.TA.26-655-j19-1.12.R', '00:03:00'), <br>\n",
    "(u'8503376', '10:00:00', 'Terminus', '00:00:00')]\n",
    "\n",
    "\n",
    "terminus_success_probability(<br> \n",
    "last_stop_id = u'8503376',<br> \n",
    "last_trip_id = u'39.TA.26-7-A-j19-1.12.H',<br> \n",
    "last_trip_arrival_time = '09:37:00',<br> \n",
    "terminus_stop_id = u'8503376',<br> \n",
    "terminus_time = '10:00:00')\n",
    "\n",
    "__B.2. Terminus transfer with walk__\n",
    "\n",
    "(u'8575944', '09:14:00', u'156.TA.26-650-j19-1.7.H', '00:02:00'), <br>\n",
    "(u'8503374', '09:24:53', 'walk', '00:07:06'), <br>\n",
    "(u'8503381',  '10:00:00', 'Terminus', '00:00:00')]\n",
    "\n",
    "terminus_success_probability(<br> \n",
    "last_stop_id = u'8503374',<br> \n",
    "last_trip_id = u'156.TA.26-650-j19-1.7.H',<br> \n",
    "last_trip_arrival_time = '09:16:00',<br> \n",
    "terminus_stop_id = u'8503376',<br> \n",
    "terminus_time = '10:00:00')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hdfs dfs -cat /user/lortkipa/period_prediction_df.pkl\n",
      "Success\n",
      "Running system command: hdfs dfs -cat /user/lortkipa/stop_prediction_df.pkl\n",
      "Success\n",
      "Running system command: hdfs dfs -cat /user/lortkipa/full_prediction_df.pkl\n",
      "Success"
     ]
    }
   ],
   "source": [
    "# Prediction dataframes\n",
    "period_pred = get_pred_file('/user/lortkipa/period_prediction_df.pkl')\n",
    "stop_pred = get_pred_file('/user/lortkipa/stop_prediction_df.pkl')\n",
    "full_pred = get_pred_file('/user/lortkipa/full_prediction_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hdfs dfs -cat /user/lortkipa/filtered_stops_Premoved.pkl\n",
      "Success"
     ]
    }
   ],
   "source": [
    "# stops and their names\n",
    "stops = read_hdfs('/user/lortkipa/filtered_stops_Premoved.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We first try multiple schedule difference to see if the increase time for transfer has the right impact on probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule difference: 60\n",
      "Walking time: 17.1925633418\n",
      "Transfer_time_surplus: -77.1925633418\n",
      "Mean arrival delay: 92.391102873\n",
      "Mean departure delay: 123.865744606\n",
      "0.3836701579288434"
     ]
    }
   ],
   "source": [
    "# One minute transfer\n",
    "transfer_success_probability(stop1_id='8503304', trip1_id='39.TA.26-7-A-j19-1.12.H', trip1_arrival_time='08:50:00', \n",
    "                             stop2_id='8575944', trip2_id='156.TA.26-650-j19-1.7.H', trip2_departure_time='08:51:00', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule difference: 300\n",
      "Walking time: 17.1925633418\n",
      "Transfer_time_surplus: 162.807436658\n",
      "Mean arrival delay: 92.391102873\n",
      "Mean departure delay: 123.865744606\n",
      "0.8956707945869421"
     ]
    }
   ],
   "source": [
    "# Five minutes transfer\n",
    "transfer_success_probability(stop1_id='8503304', trip1_id='39.TA.26-7-A-j19-1.12.H', trip1_arrival_time='08:50:00', \n",
    "                             stop2_id='8575944', trip2_id='156.TA.26-650-j19-1.7.H', trip2_departure_time='08:55:00', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule difference: 600\n",
      "Walking time: 17.1925633418\n",
      "Transfer_time_surplus: 462.807436658\n",
      "Mean arrival delay: 92.391102873\n",
      "Mean departure delay: 123.865744606\n",
      "0.9993096637863208"
     ]
    }
   ],
   "source": [
    "# Ten minutes transfer\n",
    "transfer_success_probability(stop1_id='8503304', trip1_id='39.TA.26-7-A-j19-1.12.H', trip1_arrival_time='08:50:00', \n",
    "                             stop2_id='8575944', trip2_id='156.TA.26-650-j19-1.7.H', trip2_departure_time='09:00:00', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule difference: 600\n",
      "Walking time: 2782.60558465\n",
      "Transfer_time_surplus: -2302.60558465\n",
      "Mean arrival delay: 92.391102873\n",
      "Mean departure delay: 123.865744606\n",
      "3.357404360338517e-49"
     ]
    }
   ],
   "source": [
    "## Test for very far stations\n",
    "transfer_success_probability(stop1_id='8503304', trip1_id='39.TA.26-7-A-j19-1.12.H', trip1_arrival_time='08:50:00', \n",
    "                             stop2_id='8575946', trip2_id='156.TA.26-650-j19-1.7.H', trip2_departure_time='09:00:00', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule difference: -3000\n",
      "Walking time: 0.0\n",
      "Transfer_time_surplus: -3120.0\n",
      "Mean arrival delay: 92.391102873\n",
      "Mean departure delay: 128.27618165\n",
      "4.5845366858185324e-85"
     ]
    }
   ],
   "source": [
    "# Test for arrival time after departure time\n",
    "transfer_success_probability(stop1_id='8503304', trip1_id='39.TA.26-7-A-j19-1.12.H', trip1_arrival_time='09:50:00', \n",
    "                             stop2_id='8503304', trip2_id='156.TA.26-650-j19-1.7.H', trip2_departure_time='09:00:00', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999998929"
     ]
    }
   ],
   "source": [
    "# Test for terminus station\n",
    "terminus_success_probability(last_stop_id = u'8503374',\n",
    "                             last_trip_id = u'156.TA.26-650-j19-1.7.H',\n",
    "                             last_trip_arrival_time = '09:16:00',\n",
    "                             terminus_stop_id = u'8503376',\n",
    "                             terminus_time = '10:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
