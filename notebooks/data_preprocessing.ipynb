{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "\n",
    "This notebook contains data cleaning and preparation procedure.\n",
    "\n",
    "### 1) stops.txt\n",
    "\n",
    "We read stops file. We select all the unique stations (without platform information). Next we filter on radius and keep only stations that are withing 15km from Zurich main station and we save the final list of stops <b>filtered_stops</b> for future usage as '/user/lortkipa/filtered_stops_Premoved.pkl'. We are left with approximately 1600 stations.\n",
    "\n",
    "\n",
    "### 2) calendar.txt, trips.txt \n",
    "\n",
    "Goal: Leave only services operating Mon-Fri. We read calendar.txt and select services <b>allowed_service_ids</b> that correspond to operation from Mo-Fri. Next we select trips <b>allowed_trips_ids</b> that belong to service in allowed_service_ids. We will use this information for final filtering described below.\n",
    "\n",
    "\n",
    "### 3) stop_times.txt\n",
    "\n",
    "Goal of this part is to read stop_times file and only keep entries of stops/station that are within 15km of Zurich using <b>filtered_stops</b>, have trips that are allowed according to <b>allowed_trips_ids</b> described above. Furthermore, we only keep entries concerning normal time period, from 06:00 to 20:00. <b>filtered_stop_time</b> contains final filtered content of stop_times document.\n",
    "\n",
    "### 4) current_next\n",
    "\n",
    "Here we join <b>filtered_stop_time</b> on itself. We want that each row of the new table contains information of a stations and its next station according to their route. For that reaseon we join <b>filtered_stop_time</b> on itself, where trip_ids are the same and stop_sequence of the first one equal to the stop_sequence-1 for the next one. This way we will have a row corresponding to a station and its next station according to the trip_id they are on. Joined table <b>current_next</b> is saved at 'hdfs:/user/lortkipa/current_next_6_22_Pcor.parquet' and will be used for transport graph generation. For more details of how individual rows look like see data_preprocessing.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Content for the notebook:\n",
    "\n",
    "0. Helper Functions\n",
    "1. Data reading/pre-processing\n",
    "    - stops.txt\n",
    "    - calendar.txt, trips.txt\n",
    "    - stop_times.txt\n",
    "    - current_next dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.app.name': 'my-awesome-group_final'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>6287</td><td>application_1589299642358_0776</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0776/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0776_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>6288</td><td>application_1589299642358_0777</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0777/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0777_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>6291</td><td>application_1589299642358_0780</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0780/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0780_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>6292</td><td>application_1589299642358_0781</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0781/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0781_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>6293</td><td>application_1589299642358_0782</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0782/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0782_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>6295</td><td>application_1589299642358_0784</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0784/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0784_01_000001/ebouille\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{\"conf\": {\n",
    "    \"spark.app.name\": \"my-awesome-group_final\"\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>6296</td><td>application_1589299642358_0785</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0785/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0785_01_000001/ebouille\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f196b2d8750>"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Helper Functions / Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "from geopy import distance as dist\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# global parameters\n",
    "start_day = u'06:00:00'\n",
    "end_day   = u'22:00:00' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import pandas as pd\n",
    "from hdfs3 import HDFileSystem\n",
    "\n",
    "# configuring connection with hdfs system\n",
    "hdfs = HDFileSystem(host='hdfs://iccluster044.iccluster.epfl.ch',port=8020, user='ebouille')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cell to communicate with hdfs\n",
    "import subprocess, pickle\n",
    "\n",
    "def run_cmd(args_list):\n",
    "    \"\"\"Run linux commands.\"\"\"\n",
    "    print('Running system command: {0}'.format(' '.join(args_list)))    \n",
    "    proc = subprocess.Popen(args_list,                            \n",
    "                            stdout=subprocess.PIPE,                            \n",
    "                            stderr=subprocess.PIPE)    \n",
    "    s_output, s_err = proc.communicate()    \n",
    "    s_return =  proc.returncode\n",
    "    return s_return, s_output, s_err\n",
    "\n",
    "\n",
    "def save_hdfs(localPath, hdfsPath):\n",
    "    \n",
    "    (ret, out, err)= run_cmd(['hdfs','dfs','-put','-f', localPath, hdfsPath])\n",
    "    if err:\n",
    "        print(err)\n",
    "    else:\n",
    "        print('Success')\n",
    "        \n",
    "def read_hdfs(hdfsPath):\n",
    "    \n",
    "    (ret, out, err)= run_cmd(['hdfs','dfs','-cat', hdfsPath])\n",
    "    if err:\n",
    "        print(err)\n",
    "    else:\n",
    "        print('Success')\n",
    "    return pickle.loads(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as functions\n",
    "\n",
    "# keep the parent ID only\n",
    "@functions.udf\n",
    "def keep_parent_ID(text):\n",
    "    \"\"\"Keep only parent ID and remove trailing P if necessary\"\"\"\n",
    "    parent_id = text.split(':')[0]\n",
    "    \n",
    "    # remove trailing P if present\n",
    "    if parent_id[-1] == 'P':\n",
    "        parent_id = parent_id[:-1]\n",
    "        \n",
    "    return parent_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read needed datasets and filter them accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read Stops File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read stops file. We select all the unique stations (without platform information). Next we filter on radius and keep only stations that are withing 15km from Zurich main station and we save the final list of stops <b>filtered_stops</b> for future usage as '/user/lortkipa/filtered_stops_Premoved.pkl'. We are left with approximately 1600 stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- stop_name: string (nullable = true)\n",
      " |-- stop_lat: string (nullable = true)\n",
      " |-- stop_lon: string (nullable = true)\n",
      " |-- location_type: string (nullable = true)\n",
      " |-- parent_station: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "# reading stops file\n",
    "stops = spark.read.csv('hdfs:/data/sbb/timetables/csv/stops/2019/05/14/stops.txt', header=True)\n",
    "stops.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(stop_id=u'8004238'), Row(stop_id=u'8004238:0:1'), Row(stop_id=u'8004238:0:2'), Row(stop_id=u'8014008'), Row(stop_id=u'8014008:0:10'), Row(stop_id=u'8014008:0:11'), Row(stop_id=u'8014008:0:5'), Row(stop_id=u'8014008:0:7b'), Row(stop_id=u'8014008:0:9'), Row(stop_id=u'8014008:0:9b'), Row(stop_id=u'8014015'), Row(stop_id=u'8014015:0:1'), Row(stop_id=u'8014015:0:2'), Row(stop_id=u'8014020:0:2'), Row(stop_id=u'8014021'), Row(stop_id=u'8014021:0:2'), Row(stop_id=u'8014021:0:3'), Row(stop_id=u'8014021:0:4'), Row(stop_id=u'8014021:0:5'), Row(stop_id=u'8014021:0:9'), Row(stop_id=u'8014031:0:2'), Row(stop_id=u'8014033:0:1'), Row(stop_id=u'8014035:0:1'), Row(stop_id=u'8014035:0:2'), Row(stop_id=u'8014036:0:2'), Row(stop_id=u'8014037:0:1'), Row(stop_id=u'8014037:0:2'), Row(stop_id=u'8014038:0:1'), Row(stop_id=u'8014039:0:2'), Row(stop_id=u'8014040:0:2b')]"
     ]
    }
   ],
   "source": [
    "stops.where(col('parent_station').isNotNull()).select('stop_id').take(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30631\n",
      "25784\n",
      "   stop_id               stop_name  ... location_type parent_station\n",
      "0  1322058        Migiandone Bivio  ...          None           None\n",
      "1  8014584   Konstanz-Petershausen  ...          None       8014584P\n",
      "2  8029549                   Aalen  ...          None       8029549P\n",
      "3  8500383        Kappel SO, Kreuz  ...          None           None\n",
      "4  8500737  Mont-sur-Rolle, église  ...          None           None\n",
      "\n",
      "[5 rows x 6 columns]"
     ]
    }
   ],
   "source": [
    "print(stops.count())\n",
    "# remove platform information and drop duplicates\n",
    "stops = stops.withColumn(\"stop_id\", keep_parent_ID(stops[\"stop_id\"])).dropDuplicates([\"stop_id\"])\n",
    "print(stops.count())\n",
    "# convert to pandas\n",
    "df_stops = stops.toPandas().dropna(0, subset=['stop_id']).reset_index().drop('index', axis=1)\n",
    "df_stops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining stops: 1583"
     ]
    }
   ],
   "source": [
    "# keep only stations within 15kms of Zurich Main Station (47.378177, 8.540192)\n",
    "zurich_coordinate = df_stops[df_stops['stop_id']=='8503000']\n",
    "z_lat = zurich_coordinate['stop_lat'].values[0]\n",
    "z_lon = zurich_coordinate['stop_lon'].values[0]\n",
    "\n",
    "# Filters stations outside of 15k.\n",
    "filtered_stops = df_stops[df_stops.apply(lambda x: dist.distance(\n",
    "                   (float(z_lat), \n",
    "                   float(z_lon)),\n",
    "                   (float(x['stop_lat']), \n",
    "                   float(x['stop_lon']))).km <= 15, axis=1)]\n",
    "\n",
    "print('Remaining stops: %d'%len(filtered_stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hdfs dfs -put -f filtered_stops_Premoved.pkl /user/lortkipa/\n",
      "Success"
     ]
    }
   ],
   "source": [
    "# save to pickle\n",
    "filtered_stops.to_pickle('filtered_stops_Premoved.pkl')\n",
    "del filtered_stops\n",
    "# send to hdfs\n",
    "save_hdfs('filtered_stops_Premoved.pkl','/user/lortkipa/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hdfs dfs -cat /user/lortkipa/filtered_stops_Premoved.pkl\n",
      "Success\n",
      "    stop_id               stop_name  ... location_type parent_station\n",
      "10  8502508  Spreitenbach, Raiacker  ...          None           None\n",
      "14  8503078                Waldburg  ...          None           None\n",
      "\n",
      "[2 rows x 6 columns]"
     ]
    }
   ],
   "source": [
    "filtered_stops = read_hdfs('/user/lortkipa/filtered_stops_Premoved.pkl')\n",
    "filtered_stops.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Read calendar and trips files\n",
    "\n",
    "Goal: Leave only services operating Mon-Fri. We read calendar.txt and select services <b>allowed_service_ids</b> that correspond to operation from Mo-Fri. Next we select trips <b>allowed_trips_ids</b> that belong to service in allowed_service_ids. We will use this information for final filtering described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- service_id: string (nullable = true)\n",
      " |-- monday: string (nullable = true)\n",
      " |-- tuesday: string (nullable = true)\n",
      " |-- wednesday: string (nullable = true)\n",
      " |-- thursday: string (nullable = true)\n",
      " |-- friday: string (nullable = true)\n",
      " |-- saturday: string (nullable = true)\n",
      " |-- sunday: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "calendar = spark.read.csv('/data/sbb/timetables/csv/calendar/2019/05/14/calendar.txt', header=True)\n",
    "calendar.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- route_id: string (nullable = true)\n",
      " |-- service_id: string (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- trip_headsign: string (nullable = true)\n",
      " |-- trip_short_name: string (nullable = true)\n",
      " |-- direction_id: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "trips = spark.read.csv('/data/sbb/timetables/csv/trips/2019/05/14/trips.txt', header=True)\n",
    "trips.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22615\n",
      "[Row(service_id=u'TA+b0nx9', monday=u'1', tuesday=u'1', wednesday=u'1', thursday=u'1', friday=u'1', saturday=u'0', sunday=u'0', start_date=u'20181209', end_date=u'20191214')]"
     ]
    }
   ],
   "source": [
    "print(calendar.count())\n",
    "calendar.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8648"
     ]
    }
   ],
   "source": [
    "# Retrieve service_ids that we want to keep\n",
    "allowed_service_ids = calendar.where((calendar['monday'] == '1') & (calendar['tuesday'] == '1') & (\n",
    "    calendar['wednesday'] == '1') & (calendar['thursday'] == '1') & (calendar['friday'] == '1')).select('service_id')\n",
    "print(allowed_service_ids.count())\n",
    "allowed_service_ids = allowed_service_ids.rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1017413\n",
      "528368"
     ]
    }
   ],
   "source": [
    "# select trip_ids that have allowed service_id\n",
    "print(trips.select('trip_id').distinct().count())\n",
    "allowed_trips_ids = trips.where(trips['service_id'].isin(allowed_service_ids)).select('trip_id').distinct()\n",
    "print(allowed_trips_ids.count())\n",
    "#allowed_trips_ids = allowed_trips_ids.rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(trip_id=u'237.TA.1-14-j19-1.4.H')]"
     ]
    }
   ],
   "source": [
    "allowed_trips_ids.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Read Stop Times File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of this part is to read stop_times file and only keep entries of stops/station that are within 15km of Zurich using <b>filtered_stops</b>, have trips that are allowed according to <b>allowed_trips_ids</b> described above. Furthermore, we only keep entries concerning normal time period, from 06:00 to 20:00. <b> filtered_stop_time </b> contains final filtered content of stop_times document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(trip_id=u'1.TA.1-1-B-j19-1.1.R', arrival_time=u'04:20:00', departure_time=u'04:20:00', stop_id=u'8500010:0:3', stop_sequence=u'1', pickup_type=u'0', drop_off_type=u'0')]\n",
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- arrival_time: string (nullable = true)\n",
      " |-- departure_time: string (nullable = true)\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- stop_sequence: string (nullable = true)\n",
      " |-- pickup_type: string (nullable = true)\n",
      " |-- drop_off_type: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "stop_times = spark.read.csv('/data/sbb/timetables/csv/stop_times/2019/05/14/stop_times.txt', header=True)\n",
    "print(stop_times.take(1))\n",
    "stop_times.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(trip_id=u'2.TA.9-NC-j19-1.2.H', count=69), Row(trip_id=u'4.TA.9-NC-j19-1.2.H', count=69), Row(trip_id=u'3.TA.9-NC-j19-1.2.H', count=69)]"
     ]
    }
   ],
   "source": [
    "stop_times.groupBy(['trip_id']).count().sort('count', ascending=False).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- arrival_time: string (nullable = true)\n",
      " |-- departure_time: string (nullable = true)\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- stop_sequence: integer (nullable = true)\n",
      " |-- pickup_type: string (nullable = true)\n",
      " |-- drop_off_type: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "# convert stop sequence to integer\n",
    "stop_times = stop_times.withColumn(\"stop_sequence\", stop_times[\"stop_sequence\"].cast(IntegerType()))\n",
    "# remove platform etc. information and keep only main (parent) station id\n",
    "stop_times = stop_times.withColumn(\"stop_id\", keep_parent_ID(stop_times[\"stop_id\"]))\n",
    "stop_times.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11128930\n",
      "[Row(trip_id=u'1.TA.1-1-B-j19-1.1.R', arrival_time=u'04:20:00', departure_time=u'04:20:00', stop_id=u'8500010', stop_sequence=1, pickup_type=u'0', drop_off_type=u'0'), Row(trip_id=u'1.TA.1-1-B-j19-1.1.R', arrival_time=u'04:24:00', departure_time=u'04:24:00', stop_id=u'8500020', stop_sequence=2, pickup_type=u'0', drop_off_type=u'0'), Row(trip_id=u'1.TA.1-1-B-j19-1.1.R', arrival_time=u'04:28:00', departure_time=u'04:28:00', stop_id=u'8500021', stop_sequence=3, pickup_type=u'0', drop_off_type=u'0'), Row(trip_id=u'1.TA.1-1-B-j19-1.1.R', arrival_time=u'04:30:00', departure_time=u'04:30:00', stop_id=u'8517131', stop_sequence=4, pickup_type=u'0', drop_off_type=u'0'), Row(trip_id=u'1.TA.1-1-B-j19-1.1.R', arrival_time=u'04:32:00', departure_time=u'04:32:00', stop_id=u'8500300', stop_sequence=5, pickup_type=u'0', drop_off_type=u'0')]"
     ]
    }
   ],
   "source": [
    "print(stop_times.count())\n",
    "stop_times.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2319312\n",
      "[Row(trip_id=u'2.TA.1-1-E-j19-1.1.H', arrival_time=u'06:01:00', departure_time=u'06:01:00', stop_id=u'8578679', stop_sequence=24, pickup_type=u'0', drop_off_type=u'0'), Row(trip_id=u'3.TA.1-1-E-j19-1.2.R', arrival_time=u'20:08:00', departure_time=u'20:08:00', stop_id=u'8578679', stop_sequence=1, pickup_type=u'0', drop_off_type=u'0'), Row(trip_id=u'3.TA.1-1-E-j19-1.2.R', arrival_time=u'20:08:00', departure_time=u'20:08:00', stop_id=u'8590314', stop_sequence=2, pickup_type=u'0', drop_off_type=u'0')]\n",
      "1870681\n",
      "361520"
     ]
    }
   ],
   "source": [
    "# Selecting stop times of only the stops that are in our filtered stop list (15 km radius from Zurich HB)\n",
    "filtered_stop_time = stop_times.where(col('stop_id').isin(list(filtered_stops['stop_id'].values)))\n",
    "print(filtered_stop_time.count())\n",
    "\n",
    "# selecting stop time that operate only through accepted times\n",
    "filtered_stop_time = filtered_stop_time.where((col('arrival_time') > start_day) & (col('departure_time') < end_day))\n",
    "print(filtered_stop_time.take(3))\n",
    "print(filtered_stop_time.count())\n",
    "\n",
    "# leave trips corresponding to service id-s that run Mo-Fri\n",
    "#filtered_stop_time = filtered_stop_time.where(col('trip_id').isin(allowed_trips_ids))\n",
    "filtered_stop_time = filtered_stop_time.join(\n",
    "    allowed_trips_ids, ['trip_id']).cache()\n",
    "print(filtered_stop_time.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(trip_id=u'78.TA.1-1-E-j19-1.12.R', arrival_time=u'17:34:00', departure_time=u'17:34:00', stop_id=u'8578679', stop_sequence=1, pickup_type=u'0', drop_off_type=u'0')]"
     ]
    }
   ],
   "source": [
    "filtered_stop_time.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Stops Remaining:', 1406)"
     ]
    }
   ],
   "source": [
    "print('Stops Remaining:', filtered_stop_time.select('stop_id').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Create joined current_next Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we join <b>filtered_stop_time</b> on itself. We want that each row of the new table contains information of a stations and its next station according to their route. For that reaseon we join <b>filtered_stop_time</b> on itself, where trip_ids are the same and stop_sequence of the first one equal to the stop_sequence-1 for the next one. This way we will have a row corresponding to a station and its next station according to the trip_id they are on. Joined table <b>current_next</b> is saved at 'hdfs:/user/lortkipa/current_next_6_22_Pcor.parquet' and will be used for transport graph generation. For more details of how individual rows look like see data_preprocessing.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# self join stop times file so that each row contains infromation of a station and its next stations according\n",
    "# to their route\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "t1 = filtered_stop_time.alias('t1')\n",
    "\n",
    "t2 = (filtered_stop_time\n",
    "      .withColumn('trip_id_2', col('trip_id'))\n",
    "      .withColumn('arrival_time_2', col('arrival_time'))\n",
    "      .withColumn('departure_time_2', col('departure_time'))\n",
    "      .withColumn('stop_id_2', col('stop_id'))\n",
    "      .withColumn('stop_sequence_2', col('stop_sequence'))\n",
    "      .withColumn('stop_sequence_adjusted', col('stop_sequence') -1)\n",
    "      .select('trip_id_2','arrival_time_2','departure_time_2','stop_id_2', 'stop_sequence_2', 'stop_sequence_adjusted'))\n",
    "\n",
    "# join current station with its next stations defined by their trip_it and stop_sequence\n",
    "current_next = t1.join(t2, (t1.trip_id == t2.trip_id_2) & (t1.stop_sequence == t2.stop_sequence_adjusted)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the Generated File for Future use\n",
    "#current_next.write.parquet('hdfs:/user/lortkipa/current_next_8_18.parquet')\n",
    "current_next.write.parquet('hdfs:/user/lortkipa/current_next_6_22_Pcor.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(trip_id=u'1356.TA.26-31-j19-1.12.R', arrival_time=u'16:21:00', departure_time=u'16:21:00', stop_id=u'8591186', stop_sequence=1, pickup_type=u'0', drop_off_type=u'0', trip_id_2=u'1356.TA.26-31-j19-1.12.R', arrival_time_2=u'16:22:00', departure_time_2=u'16:22:00', stop_id_2=u'8591334', stop_sequence_2=2, stop_sequence_adjusted=1)]\n",
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- arrival_time: string (nullable = true)\n",
      " |-- departure_time: string (nullable = true)\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- stop_sequence: integer (nullable = true)\n",
      " |-- pickup_type: string (nullable = true)\n",
      " |-- drop_off_type: string (nullable = true)\n",
      " |-- trip_id_2: string (nullable = true)\n",
      " |-- arrival_time_2: string (nullable = true)\n",
      " |-- departure_time_2: string (nullable = true)\n",
      " |-- stop_id_2: string (nullable = true)\n",
      " |-- stop_sequence_2: integer (nullable = true)\n",
      " |-- stop_sequence_adjusted: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "## Read Saved File\n",
    "current_next = spark.read.parquet('hdfs:/user/lortkipa/current_next_6_22_Pcor.parquet')\n",
    "print(current_next.take(1))\n",
    "current_next.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see on the example above first part of the row corresponding to stop 1 and the second part to stop 2 of the '1356.TA.26-31-j19-1.12.R' trip."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
